---
title: 'Chapter 10(Edition 8): 10.7, 10.8, 10.10'
author: "Jeremy Ling & Emmanuel Mejia"
date: "May 15, 2018"
output: pdf_document
---

```{r,  warning=FALSE, message=FALSE}
# loading libraries
library(car)
library(gplots)
#code function
coded=function(x) #a function to code variable x
{
  ifelse(x=="+", 1, -1)
}
```

##10.7

\textbf{The brake horsepower developed by an automobile
engine on a dynamometer is thought to be a function of the
engine speed in revolutions per minute (rpm), the road octane
number of the fuel, and the engine compression. An experiment
is run in the laboratory and the data that follow are
collected:}

```{r}
# creating data table
Brake.HP = c(225,212,229,222,219,278,246,237,233,224,223,230)
RPM = c(2000,1800,2400,1900,1600,2500,3000,3200,2800,3400,1800,2500)
RON = c(90,94,88,91,86,96,94,90,88,86,90,89)
Compression = c(100,95,110,96,100,110,98,100,105,97,100,104)
automob = data.frame(Brake.HP,RPM,RON,Compression)
```

\textbf{(a) Fit a multiple regression model to these data.}

```{r}
# linear regression
brake.lm <- lm(formula = Brake.HP ~ RPM + RON + Compression,
               data = automob)
```

\textbf{(b) Test for significance of regression. What conclusions
can you draw?}

```{r}
# summary output
summary(brake.lm)
```
To check for regression significance, we test for the following:

$H_0: \beta_1 = \dots = \beta_p = 0 \\$
$H_1: \beta_i \neq 0,$ where $i = i, \dots, p$

From our summary output, we find that the p-value for our F test is .00317.  Rejecting the null hypothesis at $\alpha = .05$, we find that at least one of the regressors is significant in our model.

\textbf{(c) Based on t-tests, do you need all three regressor variables
in the model?}
To determine whether or not our regressors are statistically significant to our model, we test for the following:

$H_0: \beta_i = 0 \\$
$H_1: \beta_i \neq 0$

For each regressor, we reject the null hypothesis and conclude that we need all three regressor variables in the model.

##10.8

\textbf{Analyze the residuals from the regression model in
Problem 10.7. Comment on model adequacy.}

```{r}
# defining residuals
res <-automob$Brake.HP - fitted(brake.lm)

# plotting residuals
plot(fitted(brake.lm),res)

plot(automob$RPM,res)
plot(automob$RON,res)
plot(automob$Compression,res)

# checking normality
qqPlot(res)
```

After analyizing our residuals. We see that residuals for our model is good because there is no pattern and that we have random residuals displayed. We check the residuals for our predictors and all residuals are random and patternless. We quickly check normality and we seem some residuals are slighly off, normality may be off but because of the residuals our model is good.

##10.10

\textbf{Consider the $2^4$ factorial experiment in Example 6.2.
Suppose that the last observation is missing. Reanalyze the
data and draw conclusions. How do these conclusions compare
with those from the original example?}

```{r}
A <- rep(x = c("-", "+"), times = 8)
B <- rep(x = c("-","+"), each = 2, times = 4)
C <- rep(x = c("-","+"), each = 4, times = 2)
D <- rep(x = c("-","+"), each = 8)
FilRate <- c(45,71,48,65,68,60,80,65,43,100,45,104,75,86,70,96)
chemical <- data.frame(A,B,C,D,FilRate)
```

```{r, fig.show="hide"}
qqnorm(aov(FilRate~A*B*C*D, chemical), label = TRUE)#AC AD
```

```{r}
#regular Linear Regression
chem.lm <- lm(FilRate ~ A*C + A*D, chemical); summary(chem.lm)
```

```{r, fig.show="hide"}
chemicalMissO <- chemical[-c(16),]
qqnorm(aov(FilRate ~ A*B*C*D, chemicalMissO), label = TRUE)#AC AD
```

```{r}
#missing observation Linear Regression
chemmo.lm <- lm(FilRate ~ A*C + A*D, chemicalMissO); summary(chemmo.lm)
```